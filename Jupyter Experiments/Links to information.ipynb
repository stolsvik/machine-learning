{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentations with RBMs found online\n",
    "This github code:\n",
    "* https://github.com/Cospel/rbm-ae-tf\n",
    ".. was evidently inspired by these two gists\n",
    "* https://gist.github.com/myme5261314/005ceac0483fc5a581cc\n",
    "* https://gist.github.com/saliksyed/593c950ba1a3b9dd08d5\n",
    "\n",
    "# RBMs\n",
    "* https://deeplearning4j.org/restrictedboltzmannmachine\n",
    "* https://deeplearning4j.org/understandingRBMs.html\n",
    "* https://deeplearning4j.org/deepbeliefnetwork.html\n",
    "\n",
    "# AutoEncoders\n",
    "DeepColor, manga lineart colorization:\n",
    "* http://kvfrans.com/coloring-and-shading-line-art-automatically-through-conditional-gans/\n",
    ".. same, Generational Adversarial Networks (GANs):\n",
    "* http://kvfrans.com/generative-adversial-networks-explained/\n",
    ".. same, Variational Autoencoders:\n",
    "* http://kvfrans.com/variational-autoencoders-explained/\n",
    "\n",
    "TensorFlow Autoencoder models:\n",
    "* https://github.com/tensorflow/models/tree/master/autoencoder\n",
    "\n",
    "Random stuff:\n",
    "* http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/\n",
    "* https://richzhang.github.io/splitbrainauto/\n",
    "* https://arxiv.org/abs/1611.09842 - Split-Brain Autoencoders, cross-channel prediction\n",
    "* https://lazyprogrammer.me/a-tutorial-on-autoencoders/\n",
    "\n",
    "\"Reducing Dimensionality of Data with Neural Networks\" - the important paper about RBM-pretrained Autoencoders:\n",
    "https://www.cs.toronto.edu/~hinton/science.pdf\n",
    "\n",
    "# Principal Component Analysis\n",
    "* https://lazyprogrammer.me/tutorial-principal-components-analysis-pca/\n",
    "\n",
    "ohh, and finally, the big point:\n",
    "# Deep Belief Network\n",
    "The clue is that generating a multi-layered (deep) AutoEncoder by ML training is hard, so we can use a RBMs to find some initial values that are good for each layer, and then fine-tune the autoencoder using normal SGD:\n",
    "* https://en.wikipedia.org/wiki/Autoencoder#Training\n",
    "This model takes the name of deep belief network.\n",
    "* https://en.wikipedia.org/wiki/Deep_belief_network\n",
    ".. which is a kind of Deep Learning training each layer by itself\n",
    "* Wikipedia Deep Learning # Deep Belief networks, somewhat tenser and more approachable: https://en.wikipedia.org/wiki/Deep_learning#Deep_belief_networks\n",
    "* Hinton \"Scholarpedia\" article: http://www.scholarpedia.org/article/Deep_belief_networks\n",
    "* https://deeplearning4j.org/deepbeliefnetwork.html (again..)\n",
    "\n",
    "# Deep Learning\n",
    "A technique to learn representations of data\n",
    "* https://en.wikipedia.org/wiki/Deep_learning\n",
    "\n",
    "* Review paper, LeCun & Hinton: http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf\n",
    "\n",
    "# Automatic Differentiation\n",
    ".. or algorithmic differentiation\n",
    "A way to calculate differentials without doing neither symbolic nor numerical differentiation. It goes through the /code/ of your function, and does algorithms on each part of the calculation, employing the chain rule heavily along the way, producing a new function that produces the differntial of the original function when called with some arguments. Dual Numbers is the magic, which I do not currently understand..\n",
    "* An extremely small basic demo of automatic/algorithmic differentiation: https://github.com/darius/sketchbook/blob/master/misc/autodiff.py\n",
    "* A corresponding small example of using symbolic differentition: https://github.com/darius/sketchbook/blob/master/misc/symbdiff.py\n",
    "\n",
    "* Hacker News item where the above came up: https://news.ycombinator.com/item?id=10821391\n",
    "\n",
    "* http://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/\n",
    "\n",
    "* https://justindomke.wordpress.com/2009/02/17/automatic-differentiation-the-most-criminally-underused-tool-in-the-potential-machine-learning-toolbox/\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Automatic_differentiation\n",
    "\n",
    "# Knowledge in a Neural Network\n",
    "* Distilling Knowledge from an advanced network to a simpler network: http://www.cs.toronto.edu/~hinton/absps/distillation.pdf\n",
    "\n",
    "# People\n",
    "* Geoffrey E. *Hinton*: http://www.cs.toronto.edu/~hinton/\n",
    "* Yann *LeCun*: http://yann.lecun.com/\n",
    "\n",
    "# Techniques\n",
    "* Dropout: http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "* Regularization: https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n",
    "# Scholarpedia, Artificial Intelligence\n",
    "* http://www.scholarpedia.org/article/Category:Artificial_Intelligence\n",
    "\n",
    "# Why are RBMs out of favour?\n",
    "As mentioned in the Deep Learning Book..\n",
    "* http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization - look for sentence \"It is. But it works. Xavier initialization was one of the big enablers of the move away from per-layer generative pre-training.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:endre-tensorflow]",
   "language": "python",
   "name": "conda-env-endre-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
